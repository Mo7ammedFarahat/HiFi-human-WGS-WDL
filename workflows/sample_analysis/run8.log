[2024-08-02 04:31:49,86] [info] Running with database db.url = jdbc:hsqldb:mem:0bafcdba-8c2a-4062-95ed-b03e42bd3b13;shutdown=false;hsqldb.tx=mvcc
[2024-08-02 04:31:51,94] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000
[2024-08-02 04:31:51,95] [info] [RenameWorkflowOptionsInMetadata] 100%
[2024-08-02 04:31:52,22] [info] Running with database db.url = jdbc:hsqldb:mem:3efea0e3-e5fa-4baf-97de-a8a44efaf466;shutdown=false;hsqldb.tx=mvcc
[2024-08-02 04:31:52,57] [info] Slf4jLogger started
[2024-08-02 04:31:52,85] [info] Workflow heartbeat configuration:
{
  "cromwellId" : "cromid-a49ef17",
  "heartbeatInterval" : "2 minutes",
  "ttl" : "10 minutes",
  "failureShutdownDuration" : "5 minutes",
  "writeBatchSize" : 10000,
  "writeThreshold" : 10000
}
[2024-08-02 04:31:53,08] [info] Metadata summary refreshing every 1 second.
[2024-08-02 04:31:53,09] [info] No metadata archiver defined in config
[2024-08-02 04:31:53,09] [info] No metadata deleter defined in config
[2024-08-02 04:31:53,10] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.
[2024-08-02 04:31:53,10] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.
[2024-08-02 04:31:53,11] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.
[2024-08-02 04:31:53,42] [info] JobRestartCheckTokenDispenser - Distribution rate: 50 per 1 seconds.
[2024-08-02 04:31:53,43] [info] JobExecutionTokenDispenser - Distribution rate: 20 per 10 seconds.
[2024-08-02 04:31:53,45] [info] SingleWorkflowRunnerActor: Version 86
[2024-08-02 04:31:53,45] [info] SingleWorkflowRunnerActor: Submitting workflow
[2024-08-02 04:31:53,49] [info] Unspecified type (Unspecified version) workflow a38bccc8-d525-45ec-bfa6-ffabf1f5962e submitted
[2024-08-02 04:31:53,50] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2ma38bccc8-d525-45ec-bfa6-ffabf1f5962e[0m
[2024-08-02 04:31:53,51] [info] 1 new workflows fetched by cromid-a49ef17: a38bccc8-d525-45ec-bfa6-ffabf1f5962e
[2024-08-02 04:31:53,52] [info] WorkflowManagerActor: Starting workflow [38;5;2ma38bccc8-d525-45ec-bfa6-ffabf1f5962e[0m
[2024-08-02 04:31:53,53] [info] WorkflowManagerActor: Successfully started WorkflowActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e
[2024-08-02 04:31:53,53] [info] Retrieved 1 workflows from the WorkflowStoreActor
[2024-08-02 04:31:53,55] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.
[2024-08-02 04:31:53,71] [info] MaterializeWorkflowDescriptorActor [[38;5;2ma38bccc8[0m]: Parsing workflow as WDL 1.0
[2024-08-02 04:31:54,81] [info] MaterializeWorkflowDescriptorActor [[38;5;2ma38bccc8[0m]: Call-to-Backend assignments: deepvariant.deepvariant_make_examples -> SLURM, sample_analysis.bcftools -> SLURM, sample_analysis.pbsv_discover -> SLURM, hiphase.run_hiphase -> SLURM, deepvariant.deepvariant_postprocess_variants -> SLURM, sample_analysis.cpg_pileup -> SLURM, sample_analysis.mosdepth -> SLURM, sample_analysis.concat_vcf -> SLURM, sample_analysis.pbmm2_align -> SLURM, sample_analysis.coverage_dropouts -> SLURM, sample_analysis.merge_bams -> SLURM, sample_analysis.trgt -> SLURM, sample_analysis.pbsv_call -> SLURM, deepvariant.deepvariant_call_variants -> SLURM, sample_analysis.paraphase -> SLURM, sample_analysis.hificnv -> SLURM
[2024-08-02 04:31:54,94] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,95] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,96] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,96] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,97] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,97] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,97] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,99] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:54,99] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:55,00] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:55,00] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:55,01] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:55,01] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:55,01] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:55,02] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:55,02] [[38;5;220mwarn[0m] SLURM [[38;5;2ma38bccc8[0m]: Key/s [preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.
[2024-08-02 04:31:58,44] [info] Not triggering log of restart checking token queue status. Effective log interval = None
[2024-08-02 04:31:58,50] [info] Not triggering log of execution token queue status. Effective log interval = None
[2024-08-02 04:31:59,27] [info] WorkflowExecutionActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e [[38;5;2ma38bccc8[0m]: Starting sample_analysis.pbmm2_align
[2024-08-02 04:32:03,46] [info] Assigned new job execution tokens to the following groups: a38bccc8: 1
[2024-08-02 04:32:05,01] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbmm2_align:0:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 04:32:05,10] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbmm2_align:0:1]: [38;5;5mset -euo pipefail

pbmm2 --version

pbmm2 align \
	--num-threads 24 \
	--sort-memory 4G \
	--preset HIFI \
	--sample HG004_1 \
	--log-level INFO \
	--sort \
	--unmapped \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/inputs/1497664899/HG004_1.bam \
	HG004_1.HG004_1.GRCh38.aligned.bam

# movie stats
extract_read_length_and_qual.py \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/inputs/1497664899/HG004_1.bam \
> HG004_1.HG004_1.read_length_and_quality.tsv

awk '{{ b=int($2/1000); b=(b>39?39:b); print 1000*b "\t" $2; }}' \
	HG004_1.HG004_1.read_length_and_quality.tsv \
	| sort -k1,1g \
	| datamash -g 1 count 1 sum 2 \
	| awk 'BEGIN {{ for(i=0;i<=39;i++) {{ print 1000*i"\t0\t0"; }} }} {{ print; }}' \
	| sort -k1,1g \
	| datamash -g 1 sum 2 sum 3 \
> HG004_1.HG004_1.read_length_summary.tsv

awk '{{ print ($3>50?50:$3) "\t" $2; }}' \
		HG004_1.HG004_1.read_length_and_quality.tsv \
	| sort -k1,1g \
	| datamash -g 1 count 1 sum 2 \
	| awk 'BEGIN {{ for(i=0;i<=60;i++) {{ print i"\t0\t0"; }} }} {{ print; }}' \
	| sort -k1,1g \
	| datamash -g 1 sum 2 sum 3 \
> HG004_1.HG004_1.read_quality_summary.tsv[0m
[2024-08-02 04:32:05,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbmm2_align:0:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbmm2@sha256:1013aa0fd5fb42c607d78bfe3ec3d19e7781ad3aa337bf84d144c61ed7d51fa1 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbmm2@sha256:1013aa0fd5fb42c607d78bfe3ec3d19e7781ad3aa337bf84d144c61ed7d51fa1 &&
    echo "successfully pulled quay.io/pacbio/pbmm2@sha256:1013aa0fd5fb42c607d78bfe3ec3d19e7781ad3aa337bf84d144c61ed7d51fa1!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbmm2_align \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/stderr \
  -t 3600 \
  -p Main \
  -c 24 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/script"
[2024-08-02 07:30:28,14] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbmm2_align:0:1]: job id: 9907478
[2024-08-02 07:30:28,27] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbmm2_align:0:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 07:30:28,27] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbmm2_align:0:1]: Status change from - to Running
[2024-08-02 07:30:29,62] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbmm2_align:0:1]: Status change from Running to Done
[2024-08-02 07:30:31,37] [info] WorkflowExecutionActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e [[38;5;2ma38bccc8[0m]: Starting sample_analysis.pbsv_discover
[2024-08-02 07:30:33,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 1
[2024-08-02 07:30:34,55] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_discover:0:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:34,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_discover:0:1]: [38;5;5mset -euo pipefail

pbsv --version

pbsv discover \
	--log-level INFO \
	--hifi \
	--tandem-repeats /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.trf.bed \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
	HG004_1.HG004_1.GRCh38.aligned.svsig.gz[0m
[2024-08-02 07:30:34,59] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_discover:0:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_discover \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0/execution/stderr \
  -t 3600 \
  -p Main \
  -c 2 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0/execution/script"
[2024-08-02 07:30:39,58] [info] 3318cd98-2697-4822-b22f-314512ea4878-SubWorkflowActor-SubWorkflow-deepvariant:-1:1 [[38;5;2m3318cd98[0m]: Starting deepvariant.deepvariant_make_examples (8 shards)
[2024-08-02 07:30:43,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 8
[2024-08-02 07:30:45,69] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:0:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,69] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:7:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,70] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:5:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,70] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:3:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,70] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:1:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,71] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:6:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,71] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:2:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,71] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:4:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:30:45,75] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:0:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 0 7 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.0.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.0.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:1:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 8 15 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.8.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.8.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:3:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 24 31 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.24.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.24.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:6:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 48 55 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.48.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.48.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:5:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 40 47 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.40.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.40.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:2:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 16 23 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.16.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.16.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:7:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 56 63 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.56.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.56.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:4:1]: [38;5;5mset -euo pipefail

mkdir example_tfrecords nonvariant_site_tfrecords

echo "DeepVariant version: $VERSION"

seq 32 39 \
| parallel \
	--jobs 8 \
	--halt 2 \
	/opt/deepvariant/bin/make_examples \
		--norealign_reads \
		--vsc_min_fraction_indels 0.12 \
		--pileup_image_width 199 \
		--track_ref_reads \
		--phase_reads \
		--partition_size=25000 \
		--max_reads_per_partition=600 \
		--alt_aligned_pileup=diff_channels \
		--add_hp_channel \
		--sort_by_haplotypes \
		--parse_sam_aux_fields \
		--min_mapping_quality=1 \
		--mode calling \
		--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
		--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
		--examples example_tfrecords/HG004_1.examples.tfrecord@64.gz \
		--gvcf nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz \
		--task {}

tar -zcvf HG004_1.32.example_tfrecords.tar.gz example_tfrecords
tar -zcvf HG004_1.32.nonvariant_site_tfrecords.tar.gz nonvariant_site_tfrecords[0m
[2024-08-02 07:30:45,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:0:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-0/execution/script"
[2024-08-02 07:30:45,80] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:1:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-1/execution/script"
[2024-08-02 07:30:45,80] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:6:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-6/execution/script"
[2024-08-02 07:30:45,81] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:3:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-3/execution/script"
[2024-08-02 07:30:45,81] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:5:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-5/execution/script"
[2024-08-02 07:30:45,81] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:7:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-7/execution/script"
[2024-08-02 07:30:45,81] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:2:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-2/execution/script"
[2024-08-02 07:30:45,81] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:4:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_make_examples \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_make_examples/shard-4/execution/script"
[2024-08-02 07:52:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_discover:0:1]: job id: 9907577
[2024-08-02 07:52:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_discover:0:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 07:52:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_discover:0:1]: Status change from - to Running
[2024-08-02 07:52:08,96] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_discover:0:1]: Status change from Running to Done
[2024-08-02 07:52:11,88] [info] WorkflowExecutionActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e [[38;5;2ma38bccc8[0m]: Starting sample_analysis.pbsv_call (14 shards)
[2024-08-02 07:52:13,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 14
[2024-08-02 07:52:13,48] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:7:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,48] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:3:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,49] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:5:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,49] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:8:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,49] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:10:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,49] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:1:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,49] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:12:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,49] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:6:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,51] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:2:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,51] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:11:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,52] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:13:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,52] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:0:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,52] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:4:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,52] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:9:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 07:52:13,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:6:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr7 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6/execution/write_lines_55f7b074bee7c76525fc949f0528e7a6.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.6.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.6.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.6.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:7:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr8 chr9 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7/execution/write_lines_9aa6dc02cefabc37deed3c914e43ed8a.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.7.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.7.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.7.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:6:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-6/execution/script"
[2024-08-02 07:52:13,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:7:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-7/execution/script"
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:12:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr19 chr20 chr21 chr22 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12/execution/write_lines_fc6eb534d6f0565b3209fa911092ceab.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.12.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.12.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.12.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:1:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr2 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1/execution/write_lines_06c7a433d254c663517e0c1727fed84d.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.1.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.1.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.1.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:13:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chrX chrY chrM chr1_KI270706v1_random chr1_KI270707v1_random chr1_KI270708v1_random chr1_KI270709v1_random chr1_KI270710v1_random chr1_KI270711v1_random chr1_KI270712v1_random chr1_KI270713v1_random chr1_KI270714v1_random chr2_KI270715v1_random chr2_KI270716v1_random chr3_GL000221v1_random chr4_GL000008v2_random chr5_GL000208v1_random chr9_KI270717v1_random chr9_KI270718v1_random chr9_KI270719v1_random chr9_KI270720v1_random chr11_KI270721v1_random chr14_GL000009v2_random chr14_GL000225v1_random chr14_KI270722v1_random chr14_GL000194v1_random chr14_KI270723v1_random chr14_KI270724v1_random chr14_KI270725v1_random chr14_KI270726v1_random chr15_KI270727v1_random chr16_KI270728v1_random chr17_GL000205v2_random chr17_KI270729v1_random chr17_KI270730v1_random chr22_KI270731v1_random chr22_KI270732v1_random chr22_KI270733v1_random chr22_KI270734v1_random chr22_KI270735v1_random chr22_KI270736v1_random chr22_KI270737v1_random chr22_KI270738v1_random chr22_KI270739v1_random chrY_KI270740v1_random chrUn_KI270302v1 chrUn_KI270304v1 chrUn_KI270303v1 chrUn_KI270305v1 chrUn_KI270322v1 chrUn_KI270320v1 chrUn_KI270310v1 chrUn_KI270316v1 chrUn_KI270315v1 chrUn_KI270312v1 chrUn_KI270311v1 chrUn_KI270317v1 chrUn_KI270412v1 chrUn_KI270411v1 chrUn_KI270414v1 chrUn_KI270419v1 chrUn_KI270418v1 chrUn_KI270420v1 chrUn_KI270424v1 chrUn_KI270417v1 chrUn_KI270422v1 chrUn_KI270423v1 chrUn_KI270425v1 chrUn_KI270429v1 chrUn_KI270442v1 chrUn_KI270466v1 chrUn_KI270465v1 chrUn_KI270467v1 chrUn_KI270435v1 chrUn_KI270438v1 chrUn_KI270468v1 chrUn_KI270510v1 chrUn_KI270509v1 chrUn_KI270518v1 chrUn_KI270508v1 chrUn_KI270516v1 chrUn_KI270512v1 chrUn_KI270519v1 chrUn_KI270522v1 chrUn_KI270511v1 chrUn_KI270515v1 chrUn_KI270507v1 chrUn_KI270517v1 chrUn_KI270529v1 chrUn_KI270528v1 chrUn_KI270530v1 chrUn_KI270539v1 chrUn_KI270538v1 chrUn_KI270544v1 chrUn_KI270548v1 chrUn_KI270583v1 chrUn_KI270587v1 chrUn_KI270580v1 chrUn_KI270581v1 chrUn_KI270579v1 chrUn_KI270589v1 chrUn_KI270590v1 chrUn_KI270584v1 chrUn_KI270582v1 chrUn_KI270588v1 chrUn_KI270593v1 chrUn_KI270591v1 chrUn_KI270330v1 chrUn_KI270329v1 chrUn_KI270334v1 chrUn_KI270333v1 chrUn_KI270335v1 chrUn_KI270338v1 chrUn_KI270340v1 chrUn_KI270336v1 chrUn_KI270337v1 chrUn_KI270363v1 chrUn_KI270364v1 chrUn_KI270362v1 chrUn_KI270366v1 chrUn_KI270378v1 chrUn_KI270379v1 chrUn_KI270389v1 chrUn_KI270390v1 chrUn_KI270387v1 chrUn_KI270395v1 chrUn_KI270396v1 chrUn_KI270388v1 chrUn_KI270394v1 chrUn_KI270386v1 chrUn_KI270391v1 chrUn_KI270383v1 chrUn_KI270393v1 chrUn_KI270384v1 chrUn_KI270392v1 chrUn_KI270381v1 chrUn_KI270385v1 chrUn_KI270382v1 chrUn_KI270376v1 chrUn_KI270374v1 chrUn_KI270372v1 chrUn_KI270373v1 chrUn_KI270375v1 chrUn_KI270371v1 chrUn_KI270448v1 chrUn_KI270521v1 chrUn_GL000195v1 chrUn_GL000219v1 chrUn_GL000220v1 chrUn_GL000224v1 chrUn_KI270741v1 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13/execution/write_lines_5fbdc80d1c97cb5bf573f410e58a8010.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.13.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.13.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.13.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:5:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr6 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5/execution/write_lines_7421405db98f31e22f6ee550882a61f2.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.5.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.5.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.5.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:2:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr3 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2/execution/write_lines_dfda83c11001d23d4a10d11ea0a210a0.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.2.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.2.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.2.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:11:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr16 chr17 chr18 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11/execution/write_lines_df17a0b8f180478104ce9d3c634f247c.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.11.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.11.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.11.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:3:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr4 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3/execution/write_lines_842fbac13b1fd533b8b8c749b4bd51cf.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.3.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.3.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.3.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:9:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr12 chr13 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9/execution/write_lines_d05991ed492c797af38e87bbe26cacf9.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.9.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.9.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.9.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:0:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr1 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0/execution/write_lines_60ad0e38f0fe61f9eef44dbc30631fb1.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.0.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.0.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.0.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:4:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr5 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4/execution/write_lines_13346e800d4c8e2a7abd4e207db1c299.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.4.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.4.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.4.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:8:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr10 chr11 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8/execution/write_lines_216110b7bef0da283a4d08eb65b34977.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.8.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.8.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.8.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:10:1]: [38;5;5mset -euo pipefail

if true; then
# pbsv has the ability to call SVs by region by using indexed signatures, but
#   if an svsig.gz file doesn't contain any signatures in the region, then
#   pbsv crashes. To avoid this, filter the svsig.gz files to only contain
#   signatures in the regions.
# This is brittle and likely to break if pbsv discover changes output format.
# Build a pattern to match; we want headers (e.g., '^#') and signature
#   records where third column matches the chromosome (e.g., '^.\t.\tchr1\t').
	pattern=$(echo chr14 chr15 \
		| sed 's/^/^.\\t.\\t/; s/ /\\t\|^.\\t.\\t/g; s/$/\\t/' \
		| echo "^#|""$(</dev/stdin)")

	for svsig in /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10/inputs/-1733768968/HG004_1.HG004_1.GRCh38.aligned.svsig.gz; do
		svsig_basename=$(basename "$svsig" .svsig.gz)
		gunzip -c "$svsig" \
			| grep -P "$pattern" \
			| bgzip -c > "${svsig_basename}.regions.svsig.gz" \
			&& echo "${svsig_basename}.regions.svsig.gz" >> svsigs.fofn
	done
else
	cp /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10/execution/write_lines_7e66b3282fb3d3d20d0f6bf9a578e1f7.tmp svsigs.fofn
fi

pbsv --version

pbsv call \
	--hifi \
	--min-sv-length 20 \
	--log-level INFO \
	--num-threads 8 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	svsigs.fofn \
	"HG004_1.GRCh38.10.pbsv.vcf"

bgzip --version

bgzip "HG004_1.GRCh38.10.pbsv.vcf"

tabix --version

tabix -p vcf "HG004_1.GRCh38.10.pbsv.vcf.gz"[0m
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:13:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-13/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:1:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-1/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:12:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-12/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:5:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-5/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:11:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-11/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:2:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-2/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:3:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-3/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:4:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-4/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:0:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-0/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:9:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-9/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:10:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-10/execution/script"
[2024-08-02 07:52:13,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:8:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446 &&
    echo "successfully pulled quay.io/pacbio/pbsv@sha256:d78ee6deb92949bdfde98d3e48dab1d871c177d48d8c87c73d12c45bdda43446!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_pbsv_call \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8 \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8 $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_call/shard-8/execution/script"
[2024-08-02 07:57:13,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:5:1]: job id: 9907598
[2024-08-02 07:57:13,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:5:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 07:57:13,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:5:1]: Status change from - to Running
[2024-08-02 07:57:14,31] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:5:1]: Status change from Running to Done
[2024-08-02 07:59:23,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:3:1]: job id: 9907602
[2024-08-02 07:59:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:3:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 07:59:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:3:1]: Status change from - to Running
[2024-08-02 07:59:24,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:3:1]: Status change from Running to Done
[2024-08-02 07:59:53,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:4:1]: job id: 9907599
[2024-08-02 07:59:53,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:10:1]: job id: 9907603
[2024-08-02 07:59:53,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:4:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 07:59:53,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:10:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 07:59:53,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:4:1]: Status change from - to Running
[2024-08-02 07:59:53,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:10:1]: Status change from - to Running
[2024-08-02 07:59:53,83] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:4:1]: Status change from Running to Done
[2024-08-02 07:59:54,68] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:10:1]: Status change from Running to Done
[2024-08-02 08:00:28,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:2:1]: job id: 9907595
[2024-08-02 08:00:28,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:2:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:00:28,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:2:1]: Status change from - to Running
[2024-08-02 08:00:29,48] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:2:1]: Status change from Running to Done
[2024-08-02 08:02:03,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:6:1]: job id: 9907592
[2024-08-02 08:02:03,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:6:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:02:03,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:6:1]: Status change from - to Running
[2024-08-02 08:02:04,69] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:6:1]: Status change from Running to Done
[2024-08-02 08:03:08,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:9:1]: job id: 9907605
[2024-08-02 08:03:08,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:9:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:03:08,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:9:1]: Status change from - to Running
[2024-08-02 08:03:09,33] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:9:1]: Status change from Running to Done
[2024-08-02 08:04:08,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:11:1]: job id: 9907600
[2024-08-02 08:04:08,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:7:1]: job id: 9907593
[2024-08-02 08:04:08,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:1:1]: job id: 9907594
[2024-08-02 08:04:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:7:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:04:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:11:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:04:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:1:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:04:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:11:1]: Status change from - to Running
[2024-08-02 08:04:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:1:1]: Status change from - to Running
[2024-08-02 08:04:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:7:1]: Status change from - to Running
[2024-08-02 08:04:08,84] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:7:1]: Status change from Running to Done
[2024-08-02 08:04:08,86] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:11:1]: Status change from Running to Done
[2024-08-02 08:04:09,74] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:1:1]: Status change from Running to Done
[2024-08-02 08:04:43,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:0:1]: job id: 9907597
[2024-08-02 08:04:43,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:0:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:04:43,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:0:1]: Status change from - to Running
[2024-08-02 08:04:44,05] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:0:1]: Status change from Running to Done
[2024-08-02 08:08:58,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:8:1]: job id: 9907604
[2024-08-02 08:08:58,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:8:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:08:58,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:8:1]: Status change from - to Running
[2024-08-02 08:08:59,69] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:8:1]: Status change from Running to Done
[2024-08-02 08:14:48,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:12:1]: job id: 9907601
[2024-08-02 08:14:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:13:1]: job id: 9907596
[2024-08-02 08:14:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:12:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:14:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:13:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:14:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:12:1]: Status change from - to Running
[2024-08-02 08:14:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:13:1]: Status change from - to Running
[2024-08-02 08:14:48,87] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:12:1]: Status change from Running to Done
[2024-08-02 08:14:49,79] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.pbsv_call:13:1]: Status change from Running to Done
[2024-08-02 08:14:52,57] [info] WorkflowExecutionActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e [[38;5;2ma38bccc8[0m]: Starting sample_analysis.concat_vcf
[2024-08-02 08:14:53,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 1
[2024-08-02 08:14:53,77] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.concat_vcf:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 08:14:53,84] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.concat_vcf:NA:1]: [38;5;5mset -euo pipefail

mkdir vcfs
while read -r input || [[ -n "${input}" ]]; do
	ln -s "${input}" vcfs
done < /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-concat_vcf/execution/write_lines_16cfc3d620d9b049cdb9278a84e72c53.tmp

find vcfs -name "*.vcf.gz" > vcf.list

bcftools --version

bcftools concat \
	--allow-overlaps \
	--threads 3 \
	--output-type z \
	--output HG004_1.GRCh38.pbsv.vcf.gz \
	--file-list vcf.list

bcftools index --tbi HG004_1.GRCh38.pbsv.vcf.gz[0m
[2024-08-02 08:14:53,85] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.concat_vcf:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/bcftools@sha256:36d91d5710397b6d836ff87dd2a924cd02fdf2ea73607f303a8544fbac2e691f | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/bcftools@sha256:36d91d5710397b6d836ff87dd2a924cd02fdf2ea73607f303a8544fbac2e691f &&
    echo "successfully pulled quay.io/pacbio/bcftools@sha256:36d91d5710397b6d836ff87dd2a924cd02fdf2ea73607f303a8544fbac2e691f!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_concat_vcf \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-concat_vcf \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-concat_vcf/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-concat_vcf/execution/stderr \
  -t 3600 \
  -p Main \
  -c 4 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-concat_vcf:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-concat_vcf $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-concat_vcf/execution/script"
[2024-08-02 08:15:08,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.concat_vcf:NA:1]: job id: 9907606
[2024-08-02 08:15:08,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.concat_vcf:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:15:08,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.concat_vcf:NA:1]: Status change from - to Running
[2024-08-02 08:15:09,04] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.concat_vcf:NA:1]: Status change from Running to Done
[2024-08-02 08:39:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:6:1]: job id: 9907583
[2024-08-02 08:39:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:6:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:39:48,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:6:1]: Status change from - to Running
[2024-08-02 08:39:49,27] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:6:1]: Status change from Running to Done
[2024-08-02 08:41:23,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:7:1]: job id: 9907580
[2024-08-02 08:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:1:1]: job id: 9907584
[2024-08-02 08:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:1:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:7:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:1:1]: Status change from - to Running
[2024-08-02 08:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:7:1]: Status change from - to Running
[2024-08-02 08:41:23,84] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:7:1]: Status change from Running to Done
[2024-08-02 08:41:23,86] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:1:1]: Status change from Running to Done
[2024-08-02 08:42:28,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:2:1]: job id: 9907585
[2024-08-02 08:42:28,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:2:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:42:28,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:2:1]: Status change from - to Running
[2024-08-02 08:42:29,48] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:2:1]: Status change from Running to Done
[2024-08-02 08:42:58,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:0:1]: job id: 9907578
[2024-08-02 08:42:58,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:0:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:42:58,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:0:1]: Status change from - to Running
[2024-08-02 08:42:59,26] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:0:1]: Status change from Running to Done
[2024-08-02 08:44:33,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:3:1]: job id: 9907579
[2024-08-02 08:44:33,16] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:5:1]: job id: 9907581
[2024-08-02 08:44:33,16] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:5:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:44:33,16] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:3:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:44:33,16] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:5:1]: Status change from - to Running
[2024-08-02 08:44:33,16] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:3:1]: Status change from - to Running
[2024-08-02 08:44:34,21] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:3:1]: Status change from Running to Done
[2024-08-02 08:44:34,51] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:5:1]: Status change from Running to Done
[2024-08-02 08:45:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:4:1]: job id: 9907582
[2024-08-02 08:45:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:4:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 08:45:08,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:4:1]: Status change from - to Running
[2024-08-02 08:45:09,20] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_make_examples:4:1]: Status change from Running to Done
[2024-08-02 08:45:12,28] [info] 3318cd98-2697-4822-b22f-314512ea4878-SubWorkflowActor-SubWorkflow-deepvariant:-1:1 [[38;5;2m3318cd98[0m]: Starting deepvariant.deepvariant_call_variants
[2024-08-02 08:45:13,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 1
[2024-08-02 08:45:13,48] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_call_variants:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 08:45:13,53] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_call_variants:NA:1]: [38;5;5mset -euo pipefail

while read -r tfrecord_tar || [[ -n "${tfrecord_tar}" ]]; do
	tar -zxvf "${tfrecord_tar}"
done < /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants/execution/write_lines_291d15c5b429e7e1101480ed32279007.tmp

deepvariant_model_path=/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants/inputs/1632764263/model.ckpt-25500

echo "DeepVariant version: $VERSION"

/opt/deepvariant/bin/call_variants \
	--outfile HG004_1.GRCh38.call_variants_output.tfrecord.gz \
	--examples "example_tfrecords/HG004_1.examples.tfrecord@64.gz" \
	--checkpoint "${deepvariant_model_path}"[0m
[2024-08-02 08:45:13,54] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_call_variants:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_call_variants \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants/execution/stderr \
  -t 3600 \
  -p Main \
  -c 32 \
  --mem-per-cpu=7000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_call_variants/execution/script"
[2024-08-02 12:54:33,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_call_variants:NA:1]: job id: 9907608
[2024-08-02 12:54:33,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_call_variants:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 12:54:33,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_call_variants:NA:1]: Status change from - to Running
[2024-08-02 12:54:34,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_call_variants:NA:1]: Status change from Running to Done
[2024-08-02 12:54:35,68] [info] 3318cd98-2697-4822-b22f-314512ea4878-SubWorkflowActor-SubWorkflow-deepvariant:-1:1 [[38;5;2m3318cd98[0m]: Starting deepvariant.deepvariant_postprocess_variants
[2024-08-02 12:54:43,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 1
[2024-08-02 12:54:43,49] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_postprocess_variants:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 12:54:43,54] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_postprocess_variants:NA:1]: [38;5;5mset -euo pipefail

while read -r nonvariant_site_tfrecord_tar || [[ -n "${nonvariant_site_tfrecord_tar}" ]]; do
	tar -zxvf "${nonvariant_site_tfrecord_tar}"
done < /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/write_lines_40a55b3f83c257cf00ed2aee36b5d189.tmp

echo "DeepVariant version: $VERSION"

/opt/deepvariant/bin/postprocess_variants \
	--vcf_stats_report=false \
	--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	--infile /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/inputs/-900073398/HG004_1.GRCh38.call_variants_output.tfrecord.gz \
	--outfile HG004_1.GRCh38.deepvariant.vcf.gz \
	--nonvariant_site_tfrecord_path "nonvariant_site_tfrecords/HG004_1.gvcf.tfrecord@64.gz" \
	--gvcf_outfile HG004_1.GRCh38.deepvariant.g.vcf.gz[0m
[2024-08-02 12:54:43,55] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_postprocess_variants:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096 &&
    echo "successfully pulled gcr.io/deepvariant-docker/deepvariant@sha256:98c3cdbc660abed420bd15ad2a671cb39dc120d54f52f1c1f7d6dc9ce5e9a096!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_3318cd98_deepvariant_postprocess_variants \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/stderr \
  -t 3600 \
  -p Main \
  -c 2 \
  --mem-per-cpu=32000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/script"
[2024-08-02 13:36:33,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_postprocess_variants:NA:1]: job id: 9907728
[2024-08-02 13:36:33,17] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_postprocess_variants:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 13:36:33,17] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_postprocess_variants:NA:1]: Status change from - to Running
[2024-08-02 13:36:34,22] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m3318cd98[0mdeepvariant.deepvariant_postprocess_variants:NA:1]: Status change from Running to Done
[2024-08-02 13:36:36,13] [info] 3318cd98-2697-4822-b22f-314512ea4878-SubWorkflowActor-SubWorkflow-deepvariant:-1:1 [[38;5;2m3318cd98[0m]: Workflow deepvariant complete. Final Outputs:
{
  "deepvariant.gvcf": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.g.vcf.gz",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.g.vcf.gz.tbi"
  },
  "deepvariant.vcf": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.vcf.gz",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.vcf.gz.tbi"
  }
}
[2024-08-02 13:36:40,15] [info] WorkflowExecutionActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e [[38;5;2ma38bccc8[0m]: Starting sample_analysis.bcftools
[2024-08-02 13:36:43,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 1
[2024-08-02 13:36:43,53] [info] Request threw an exception on attempt #1. Retrying after 517 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:44,05] [info] Request threw an exception on attempt #2. Retrying after 2295 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:46,35] [info] Request threw an exception on attempt #3. Retrying after 6218 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:49,34] [info] be49d0bc-d331-4242-8ebb-6d6438269620-SubWorkflowActor-SubWorkflow-hiphase:-1:1 [[38;5;2mbe49d0bc[0m]: Starting hiphase.run_hiphase
[2024-08-02 13:36:52,58] [info] Request method=GET uri=https://quay.io/v2/pacbio/bcftools/manifests/sha256:46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec headers=Accept: application/vnd.docker.distribution.manifest.v2+json threw an exception on attempt #4. Giving up.
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:52,58] [info] Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithHash(Some(quay.io),Some(pacbio),bcftools,sha256:46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec,DockerHashResult(sha256,46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec))
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:52,59] [info] Request threw an exception on attempt #1. Retrying after 69 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:52,66] [info] Request threw an exception on attempt #2. Retrying after 2233 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:53,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 1
[2024-08-02 13:36:53,47] [info] Request threw an exception on attempt #1. Retrying after 765 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:54,24] [info] Request threw an exception on attempt #2. Retrying after 1123 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:54,90] [info] Request threw an exception on attempt #3. Retrying after 4795 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:55,37] [info] Request threw an exception on attempt #3. Retrying after 6492 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:59,70] [info] Request method=GET uri=https://quay.io/v2/pacbio/bcftools/manifests/sha256:46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec headers=Accept: application/vnd.oci.image.index.v1+json threw an exception on attempt #4. Giving up.
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:36:59,71] [[38;5;220mwarn[0m] BackendPreparationActor_for_a38bccc8:sample_analysis.bcftools:-1:1 [[38;5;2ma38bccc8[0m]: Docker lookup failed
java.lang.Exception: Failed to get docker hash for quay.io/pacbio/bcftools@sha256:46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:222)
	at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:88)
	at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:73)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
	at akka.actor.FSM.processEvent(FSM.scala:707)
	at akka.actor.FSM.processEvent$(FSM.scala:704)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowDockerLookupActor.scala:40)
	at akka.actor.LoggingFSM.processEvent(FSM.scala:847)
	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.processEvent(WorkflowDockerLookupActor.scala:40)
	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701)
	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
	at cromwell.docker.DockerClientHelper$$anonfun$dockerResponseReceive$1.applyOrElse(DockerClientHelper.scala:16)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:269)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)
	at akka.actor.Actor.aroundReceive(Actor.scala:539)
	at akka.actor.Actor.aroundReceive$(Actor.scala:537)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.aroundReceive(WorkflowDockerLookupActor.scala:40)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614)
	at akka.actor.ActorCell.invoke(ActorCell.scala:583)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268)
	at akka.dispatch.Mailbox.run(Mailbox.scala:229)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:241)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

[2024-08-02 13:36:59,71] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.bcftools:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 13:36:59,74] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.bcftools:NA:1]: [38;5;5mset -euo pipefail

bcftools --version

bcftools stats \
	--threads 1 \
	--apply-filters PASS --samples HG004_1 \
	--fasta-ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/inputs/672755989/HG004_1.GRCh38.deepvariant.vcf.gz \
> HG004_1.GRCh38.deepvariant.vcf.stats.txt

bcftools roh \
	--threads 1 \
	--AF-dflt 0.4 \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/inputs/672755989/HG004_1.GRCh38.deepvariant.vcf.gz \
> HG004_1.GRCh38.deepvariant.bcftools_roh.out

echo -e "#chr\\tstart\\tend\\tqual" > HG004_1.GRCh38.deepvariant.roh.bed
awk -v OFS='\t' '$1=="RG" {{ print $3, $4, $5, $8 }}' \
	HG004_1.GRCh38.deepvariant.bcftools_roh.out \
>> HG004_1.GRCh38.deepvariant.roh.bed[0m
[2024-08-02 13:36:59,75] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.bcftools:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/bcftools@sha256:46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/bcftools@sha256:46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec &&
    echo "successfully pulled quay.io/pacbio/bcftools@sha256:46720a7ab5feba5be06d5269454a6282deec13060e296f0bc441749f6f26fdec!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_bcftools \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/stderr \
  -t 3600 \
  -p Main \
  -c 2 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/script"
[2024-08-02 13:37:01,86] [info] Request method=GET uri=https://quay.io/v2/pacbio/hiphase/manifests/sha256:493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8 headers=Accept: application/vnd.docker.distribution.manifest.v2+json threw an exception on attempt #4. Giving up.
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:37:01,86] [info] Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithHash(Some(quay.io),Some(pacbio),hiphase,sha256:493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8,DockerHashResult(sha256,493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8))
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:37:01,87] [info] Request threw an exception on attempt #1. Retrying after 596 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:37:02,47] [info] Request threw an exception on attempt #2. Retrying after 1616 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:37:04,09] [info] Request threw an exception on attempt #3. Retrying after 4629 milliseconds
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:37:08,72] [info] Request method=GET uri=https://quay.io/v2/pacbio/hiphase/manifests/sha256:493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8 headers=Accept: application/vnd.oci.image.index.v1+json threw an exception on attempt #4. Giving up.
org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
	at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144)
	at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139)
	at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119)
	at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83)
	at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:173)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finishConnect(UnixAsynchronousSocketChannelImpl.java:256)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.finish(UnixAsynchronousSocketChannelImpl.java:202)
	at java.base/sun.nio.ch.UnixAsynchronousSocketChannelImpl.onEvent(UnixAsynchronousSocketChannelImpl.java:217)
	at java.base/sun.nio.ch.EPollPort$EventHandlerTask.run(EPollPort.java:306)
	at java.base/sun.nio.ch.AsynchronousChannelGroupImpl$1.run(AsynchronousChannelGroupImpl.java:113)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2024-08-02 13:37:08,73] [[38;5;220mwarn[0m] BackendPreparationActor_for_be49d0bc:hiphase.run_hiphase:-1:1 [[38;5;2mbe49d0bc[0m]: Docker lookup failed
java.lang.Exception: Failed to get docker hash for quay.io/pacbio/hiphase@sha256:493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8 Error connecting to https://quay.io using address quay.io:443 (unresolved: false)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:222)
	at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:88)
	at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:73)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
	at akka.actor.FSM.processEvent(FSM.scala:707)
	at akka.actor.FSM.processEvent$(FSM.scala:704)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowDockerLookupActor.scala:40)
	at akka.actor.LoggingFSM.processEvent(FSM.scala:847)
	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.processEvent(WorkflowDockerLookupActor.scala:40)
	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701)
	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
	at cromwell.docker.DockerClientHelper$$anonfun$dockerResponseReceive$1.applyOrElse(DockerClientHelper.scala:16)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:269)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)
	at akka.actor.Actor.aroundReceive(Actor.scala:539)
	at akka.actor.Actor.aroundReceive$(Actor.scala:537)
	at cromwell.engine.workflow.WorkflowDockerLookupActor.aroundReceive(WorkflowDockerLookupActor.scala:40)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614)
	at akka.actor.ActorCell.invoke(ActorCell.scala:583)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268)
	at akka.dispatch.Mailbox.run(Mailbox.scala:229)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:241)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

[2024-08-02 13:37:08,73] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbe49d0bc[0mhiphase.run_hiphase:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 13:37:08,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbe49d0bc[0mhiphase.run_hiphase:NA:1]: [38;5;5mset -euo pipefail

hiphase --version

# phase VCFs and haplotag BAM
hiphase --threads 16 \
	--sample-name HG004_1 \
	--vcf /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/inputs/672755989/HG004_1.GRCh38.deepvariant.vcf.gz --vcf /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/inputs/-799439411/HG004_1.GRCh38.pbsv.vcf.gz \
	--output-vcf HG004_1.GRCh38.deepvariant.phased.vcf.gz --output-vcf HG004_1.GRCh38.pbsv.phased.vcf.gz \
	--bam /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/inputs/-1134348021/HG004_1.HG004_1.GRCh38.aligned.bam \
	--output-bam  HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam \
	--reference /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	--summary-file HG004_1.GRCh38.hiphase.stats.tsv \
	--blocks-file HG004_1.GRCh38.hiphase.blocks.tsv \
	--haplotag-file HG004_1.GRCh38.hiphase.haplotags.tsv \
	--global-realignment-cputime 300[0m
[2024-08-02 13:37:08,80] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbe49d0bc[0mhiphase.run_hiphase:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/hiphase@sha256:493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/hiphase@sha256:493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8 &&
    echo "successfully pulled quay.io/pacbio/hiphase@sha256:493ed4244608f29d7e2e180af23b20879c71ae3692201a610c7f1f980ee094e8!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_be49d0bc_run_hiphase \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/stderr \
  -t 3600 \
  -p Main \
  -c 16 \
  --mem-per-cpu=10000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/script"
[2024-08-02 13:38:18,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.bcftools:NA:1]: job id: 9907756
[2024-08-02 13:38:18,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.bcftools:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 13:38:18,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.bcftools:NA:1]: Status change from - to Running
[2024-08-02 13:38:19,48] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.bcftools:NA:1]: Status change from Running to Done
[2024-08-02 14:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbe49d0bc[0mhiphase.run_hiphase:NA:1]: job id: 9907757
[2024-08-02 14:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbe49d0bc[0mhiphase.run_hiphase:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 14:41:23,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbe49d0bc[0mhiphase.run_hiphase:NA:1]: Status change from - to Running
[2024-08-02 14:41:24,08] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbe49d0bc[0mhiphase.run_hiphase:NA:1]: Status change from Running to Done
[2024-08-02 14:41:29,44] [info] be49d0bc-d331-4242-8ebb-6d6438269620-SubWorkflowActor-SubWorkflow-hiphase:-1:1 [[38;5;2mbe49d0bc[0m]: Workflow hiphase complete. Final Outputs:
{
  "hiphase.phased_vcfs": "[typed object {data: \"/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.deepvariant.phased.vcf.gz\", data_index: \"/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.deepvariant.phased.vcf.gz.tbi\"}, typed object {data: \"/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.pbsv.phased.vcf.gz\", data_index: \"/scratch3/projects/003/gideonaw/PacBio_Workflow/...",
  "hiphase.hiphase_stats": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.stats.tsv",
  "hiphase.haplotagged_bams": [{
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam.bai"
  }],
  "hiphase.hiphase_blocks": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.blocks.tsv",
  "hiphase.hiphase_haplotags": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.haplotags.tsv"
}
[2024-08-02 14:41:37,57] [info] WorkflowExecutionActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e [[38;5;2ma38bccc8[0m]: Starting sample_analysis.cpg_pileup, sample_analysis.mosdepth, sample_analysis.coverage_dropouts, sample_analysis.trgt, sample_analysis.paraphase, sample_analysis.hificnv
[2024-08-02 14:41:43,45] [info] Assigned new job execution tokens to the following groups: a38bccc8: 6
[2024-08-02 14:41:44,54] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.hificnv:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 14:41:44,54] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.cpg_pileup:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 14:41:44,54] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.trgt:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 14:41:44,54] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.coverage_dropouts:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 14:41:44,55] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.paraphase:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 14:41:44,57] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.coverage_dropouts:NA:1]: [38;5;5mset -euo pipefail

# Get coverage dropouts
check_trgt_coverage.py \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts/inputs/452218010/human_GRCh38_no_alt_analysis_set.trgt.v0.3.4.bed \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts/inputs/1530078888/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam \
> HG004_1.GRCh38.trgt.dropouts.txt[0m
[2024-08-02 14:41:44,57] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.cpg_pileup:NA:1]: [38;5;5mset -euo pipefail

aligned_bam_to_cpg_scores --version

aligned_bam_to_cpg_scores \
	--threads 12 \
	--bam /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/inputs/1530078888/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam \
	--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	--output-prefix HG004_1.GRCh38 \
	--min-mapq 1 \
	--min-coverage 10 \
	--model "$PILEUP_MODEL_DIR"/pileup_calling_model.v1.tflite[0m
[2024-08-02 14:41:44,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.trgt:NA:1]: [38;5;5mset -euo pipefail

echo 

trgt --version

trgt \
	--threads 4 \
	--karyotype XX \
	--genome /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	--repeats /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/inputs/452218010/human_GRCh38_no_alt_analysis_set.trgt.v0.3.4.bed \
	--reads /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/inputs/1530078888/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam \
	--output-prefix HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt

bcftools --version

bcftools sort \
	--output-type z \
	--output HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.sorted.vcf.gz \
	HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.vcf.gz

bcftools index \
	--threads 3 \
	--tbi \
	HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.sorted.vcf.gz

samtools --version

samtools sort \
	-@ 3 \
	-o HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.spanning.sorted.bam \
	HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.spanning.bam

samtools index \
	-@ 3 \
	HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.spanning.sorted.bam[0m
[2024-08-02 14:41:44,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.paraphase:NA:1]: [38;5;5mset -euo pipefail

paraphase --version

paraphase \
	--threads 4 \
	--bam /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/inputs/1530078888/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam \
	--reference /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	--out HG004_1.paraphase[0m
[2024-08-02 14:41:44,59] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.coverage_dropouts:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/trgt@sha256:8c9f236eb3422e79d7843ffd59e1cbd9b76774525f20d88cd68ca64eb63054eb | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/trgt@sha256:8c9f236eb3422e79d7843ffd59e1cbd9b76774525f20d88cd68ca64eb63054eb &&
    echo "successfully pulled quay.io/pacbio/trgt@sha256:8c9f236eb3422e79d7843ffd59e1cbd9b76774525f20d88cd68ca64eb63054eb!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_coverage_dropouts \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts/execution/stderr \
  -t 3600 \
  -p Main \
  -c 2 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts/execution/script"
[2024-08-02 14:41:44,59] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.hificnv:NA:1]: [38;5;5mset -euo pipefail

echo 

hificnv --version

hificnv \
	--threads 8 \
	--bam /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/inputs/1530078888/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam \
	--ref /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/inputs/-1086097408/human_GRCh38_no_alt_analysis_set.fasta \
	--maf /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/inputs/1530078888/HG004_1.GRCh38.deepvariant.phased.vcf.gz \
	--exclude /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/inputs/690371160/cnv.excluded_regions.common_50.hg38.bed.gz \
	--expected-cn /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/inputs/690371160/expected_cn.hg38.XX.bed \
	--output-prefix hificnv

bcftools index --tbi hificnv.HG004_1.vcf.gz[0m
[2024-08-02 14:41:44,59] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.trgt:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/trgt@sha256:8c9f236eb3422e79d7843ffd59e1cbd9b76774525f20d88cd68ca64eb63054eb | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/trgt@sha256:8c9f236eb3422e79d7843ffd59e1cbd9b76774525f20d88cd68ca64eb63054eb &&
    echo "successfully pulled quay.io/pacbio/trgt@sha256:8c9f236eb3422e79d7843ffd59e1cbd9b76774525f20d88cd68ca64eb63054eb!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_trgt \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/stderr \
  -t 3600 \
  -p Main \
  -c 4 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/script"
[2024-08-02 14:41:44,59] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.mosdepth:NA:1]: Unrecognized runtime attribute keys: preemptible, disk, queueArn, disks, awsBatchRetryAttempts, zones
[2024-08-02 14:41:44,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.cpg_pileup:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/pb-cpg-tools@sha256:b95ff1c53bb16e53b8c24f0feaf625a4663973d80862518578437f44385f509b | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/pb-cpg-tools@sha256:b95ff1c53bb16e53b8c24f0feaf625a4663973d80862518578437f44385f509b &&
    echo "successfully pulled quay.io/pacbio/pb-cpg-tools@sha256:b95ff1c53bb16e53b8c24f0feaf625a4663973d80862518578437f44385f509b!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_cpg_pileup \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/stderr \
  -t 3600 \
  -p Main \
  -c 12 \
  --mem-per-cpu=20000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/script"
[2024-08-02 14:41:44,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.hificnv:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/hificnv@sha256:19fdde99ad2454598ff7d82f27209e96184d9a6bb92dc0485cc7dbe87739b3c2 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/hificnv@sha256:19fdde99ad2454598ff7d82f27209e96184d9a6bb92dc0485cc7dbe87739b3c2 &&
    echo "successfully pulled quay.io/pacbio/hificnv@sha256:19fdde99ad2454598ff7d82f27209e96184d9a6bb92dc0485cc7dbe87739b3c2!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_hificnv \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/stderr \
  -t 3600 \
  -p Main \
  -c 8 \
  --mem-per-cpu=20000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/script"
[2024-08-02 14:41:44,60] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.paraphase:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/paraphase@sha256:186dec5f6dabedf8c90fe381cd8f934d31fe74310175efee9ca4f603deac954d | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/paraphase@sha256:186dec5f6dabedf8c90fe381cd8f934d31fe74310175efee9ca4f603deac954d &&
    echo "successfully pulled quay.io/pacbio/paraphase@sha256:186dec5f6dabedf8c90fe381cd8f934d31fe74310175efee9ca4f603deac954d!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_paraphase \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/stderr \
  -t 3600 \
  -p Main \
  -c 4 \
  --mem-per-cpu=4000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/script"
[2024-08-02 14:41:44,61] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.mosdepth:NA:1]: [38;5;5mset -euo pipefail

mosdepth --version

mosdepth \
	--threads 3 \
	--by 500 \
	--no-per-base \
	--use-median \
	HG004_1.HG004_1.GRCh38.aligned.haplotagged \
	/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/inputs/1530078888/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam[0m
[2024-08-02 14:41:44,62] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.mosdepth:NA:1]: executing: # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
# based on the users home.
if [ -z $SINGULARITY_CACHEDIR ]; 
  then CACHE_DIR=/scratch3/users/$USER/.singularity/cache
  else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR  
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for 
# for debugging, as is the echo command. These show up in [38;5;5mstdout.submit[0m.
IMAGE=$CACHE_DIR/$(echo quay.io/pacbio/mosdepth@sha256:35d5e02facf4f38742e5cae9e5fdd3807c2b431dd8d881fd246b55e6d5f7f600 | sed 's/\//_/'g).sif
if [ ! -f $IMAGE ]; then
    flock --verbose --exclusive --timeout 900 $LOCK_FILE \
    singularity pull --force $IMAGE docker://quay.io/pacbio/mosdepth@sha256:35d5e02facf4f38742e5cae9e5fdd3807c2b431dd8d881fd246b55e6d5f7f600 &&
    echo "successfully pulled quay.io/pacbio/mosdepth@sha256:35d5e02facf4f38742e5cae9e5fdd3807c2b431dd8d881fd246b55e6d5f7f600!"
else
    echo "Image $IMAGE already exists. Yay"
fi

# Submit the script to SLURM
sbatch \
  --wait \
  -J cromwell_a38bccc8_mosdepth \
  -D /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth \
  -o /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/execution/stdout \
  -e /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/execution/stderr \
  -t 3600 \
  -p Main \
  -c 4 \
  --mem-per-cpu=8000 \
  --wrap "singularity exec --bind /scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth:/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth $IMAGE /bin/bash /cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/execution/script"
[2024-08-02 14:45:43,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.paraphase:NA:1]: job id: 9907800
[2024-08-02 14:45:43,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.paraphase:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 14:45:43,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.paraphase:NA:1]: Status change from - to Running
[2024-08-02 14:45:44,64] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.paraphase:NA:1]: Status change from Running to Done
[2024-08-02 14:46:43,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.mosdepth:NA:1]: job id: 9907795
[2024-08-02 14:46:43,15] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.mosdepth:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 14:46:43,17] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.mosdepth:NA:1]: Status change from - to Running
[2024-08-02 14:46:44,48] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.mosdepth:NA:1]: Status change from Running to Done
[2024-08-02 14:47:18,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.hificnv:NA:1]: job id: 9907794
[2024-08-02 14:47:18,15] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.hificnv:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 14:47:18,15] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.hificnv:NA:1]: Status change from - to Running
[2024-08-02 14:47:19,27] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.hificnv:NA:1]: Status change from Running to Done
[2024-08-02 14:55:53,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.trgt:NA:1]: job id: 9907797
[2024-08-02 14:55:53,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.trgt:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 14:55:53,19] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.trgt:NA:1]: Status change from - to Running
[2024-08-02 14:55:53,88] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.trgt:NA:1]: Status change from Running to Done
[2024-08-02 14:56:43,11] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.cpg_pileup:NA:1]: job id: 9907798
[2024-08-02 14:56:43,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.cpg_pileup:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 14:56:43,13] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.cpg_pileup:NA:1]: Status change from - to Running
[2024-08-02 14:56:43,72] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.cpg_pileup:NA:1]: Status change from Running to Done
[2024-08-02 15:03:18,12] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.coverage_dropouts:NA:1]: job id: 9907796
[2024-08-02 15:03:18,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.coverage_dropouts:NA:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)
[2024-08-02 15:03:18,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.coverage_dropouts:NA:1]: Status change from - to Running
[2024-08-02 15:03:19,58] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2ma38bccc8[0msample_analysis.coverage_dropouts:NA:1]: Status change from Running to Done
[2024-08-02 15:03:21,14] [info] WorkflowExecutionActor-a38bccc8-d525-45ec-bfa6-ffabf1f5962e [[38;5;2ma38bccc8[0m]: Workflow sample_analysis complete. Final Outputs:
{
  "sample_analysis.read_quality_summary": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.read_quality_summary.tsv"],
  "sample_analysis.small_variant_roh_out": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/HG004_1.GRCh38.deepvariant.bcftools_roh.out",
  "sample_analysis.paraphase_output_json": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/HG004_1.paraphase/HG004_1.json",
  "sample_analysis.aligned_bams": [{
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.GRCh38.aligned.bam",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.GRCh38.aligned.bam.bai"
  }],
  "sample_analysis.hificnv_depth_bw": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.depth.bw",
  "sample_analysis.paraphase_realigned_bam": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/HG004_1.paraphase/HG004_1_realigned_tagged.bam",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/HG004_1.paraphase/HG004_1_realigned_tagged.bam.bai"
  },
  "sample_analysis.haplotagged_bam_mosdepth_region_bed": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.regions.bed.gz",
  "sample_analysis.hiphase_blocks": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.blocks.tsv",
  "sample_analysis.trgt_dropouts": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts/execution/HG004_1.GRCh38.trgt.dropouts.txt",
  "sample_analysis.trgt_spanning_reads": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.spanning.sorted.bam",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.spanning.sorted.bam.bai"
  },
  "sample_analysis.haplotagged_bam_mosdepth_summary": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.mosdepth.summary.txt",
  "sample_analysis.bam_stats": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.read_length_and_quality.tsv"],
  "sample_analysis.hiphase_stats": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.stats.tsv",
  "sample_analysis.trgt_repeat_vcf": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.sorted.vcf.gz",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.sorted.vcf.gz.tbi"
  },
  "sample_analysis.small_variant_vcf_stats": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/HG004_1.GRCh38.deepvariant.vcf.stats.txt",
  "sample_analysis.merged_haplotagged_bam": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam.bai"
  },
  "sample_analysis.cpg_pileup_bigwigs": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-41edbd235e86bf57dad70a273e462c1c/HG004_1.GRCh38.combined.bw", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-41edbd235e86bf57dad70a273e462c1c/HG004_1.GRCh38.hap1.bw", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-41edbd235e86bf57dad70a273e462c1c/HG004_1.GRCh38.hap2.bw"],
  "sample_analysis.small_variant_roh_bed": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/HG004_1.GRCh38.deepvariant.roh.bed",
  "sample_analysis.cpg_pileup_beds": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-798cc7b4bd0f3a92f29efaad64396f99/HG004_1.GRCh38.combined.bed", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-798cc7b4bd0f3a92f29efaad64396f99/HG004_1.GRCh38.hap1.bed", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-798cc7b4bd0f3a92f29efaad64396f99/HG004_1.GRCh38.hap2.bed"],
  "sample_analysis.read_length_summary": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.read_length_summary.tsv"],
  "sample_analysis.paraphase_vcfs": "[\"/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_cfc1_hap1.vcf\", \"/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_cfc1_hap2.vcf\", \"/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_cfc1_hap3.vcf\", \"/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG00...",
  "sample_analysis.hiphase_haplotags": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.haplotags.tsv",
  "sample_analysis.small_variant_gvcf": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.g.vcf.gz",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.g.vcf.gz.tbi"
  },
  "sample_analysis.phased_small_variant_vcf": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.deepvariant.phased.vcf.gz",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.deepvariant.phased.vcf.gz.tbi"
  },
  "sample_analysis.hificnv_maf_bw": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.maf.bw",
  "sample_analysis.hificnv_vcf": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.vcf.gz",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.vcf.gz.tbi"
  },
  "sample_analysis.hificnv_copynum_bedgraph": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.copynum.bedgraph",
  "sample_analysis.svsigs": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0/execution/HG004_1.HG004_1.GRCh38.aligned.svsig.gz"],
  "sample_analysis.phased_sv_vcf": {
    "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.pbsv.phased.vcf.gz",
    "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.pbsv.phased.vcf.gz.tbi"
  }
}
[2024-08-02 15:03:23,14] [info] WorkflowManagerActor: Workflow actor for a38bccc8-d525-45ec-bfa6-ffabf1f5962e completed with status 'Succeeded'. The workflow will be removed from the workflow store.
[2024-08-02 15:04:12,75] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.
{
  "outputs": {
    "sample_analysis.read_quality_summary": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.read_quality_summary.tsv"],
    "sample_analysis.small_variant_roh_out": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/HG004_1.GRCh38.deepvariant.bcftools_roh.out",
    "sample_analysis.paraphase_output_json": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/HG004_1.paraphase/HG004_1.json",
    "sample_analysis.aligned_bams": [{
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.GRCh38.aligned.bam.bai",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.GRCh38.aligned.bam"
    }],
    "sample_analysis.hificnv_depth_bw": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.depth.bw",
    "sample_analysis.paraphase_realigned_bam": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/HG004_1.paraphase/HG004_1_realigned_tagged.bam.bai",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/HG004_1.paraphase/HG004_1_realigned_tagged.bam"
    },
    "sample_analysis.haplotagged_bam_mosdepth_region_bed": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.regions.bed.gz",
    "sample_analysis.hiphase_blocks": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.blocks.tsv",
    "sample_analysis.trgt_dropouts": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-coverage_dropouts/execution/HG004_1.GRCh38.trgt.dropouts.txt",
    "sample_analysis.trgt_spanning_reads": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.spanning.sorted.bam.bai",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.spanning.sorted.bam"
    },
    "sample_analysis.haplotagged_bam_mosdepth_summary": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-mosdepth/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.mosdepth.summary.txt",
    "sample_analysis.bam_stats": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.read_length_and_quality.tsv"],
    "sample_analysis.hiphase_stats": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.stats.tsv",
    "sample_analysis.trgt_repeat_vcf": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.sorted.vcf.gz.tbi",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-trgt/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.trgt.sorted.vcf.gz"
    },
    "sample_analysis.small_variant_vcf_stats": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/HG004_1.GRCh38.deepvariant.vcf.stats.txt",
    "sample_analysis.merged_haplotagged_bam": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam.bai",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.HG004_1.GRCh38.aligned.haplotagged.bam"
    },
    "sample_analysis.cpg_pileup_bigwigs": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-41edbd235e86bf57dad70a273e462c1c/HG004_1.GRCh38.combined.bw", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-41edbd235e86bf57dad70a273e462c1c/HG004_1.GRCh38.hap1.bw", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-41edbd235e86bf57dad70a273e462c1c/HG004_1.GRCh38.hap2.bw"],
    "sample_analysis.small_variant_roh_bed": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-bcftools/execution/HG004_1.GRCh38.deepvariant.roh.bed",
    "sample_analysis.cpg_pileup_beds": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-798cc7b4bd0f3a92f29efaad64396f99/HG004_1.GRCh38.combined.bed", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-798cc7b4bd0f3a92f29efaad64396f99/HG004_1.GRCh38.hap1.bed", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-cpg_pileup/execution/glob-798cc7b4bd0f3a92f29efaad64396f99/HG004_1.GRCh38.hap2.bed"],
    "sample_analysis.read_length_summary": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbmm2_align/shard-0/execution/HG004_1.HG004_1.read_length_summary.tsv"],
    "sample_analysis.paraphase_vcfs": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_cfc1_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_cfc1_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_cfc1_hap3.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_cfc1_variants.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ikbkg_ikbkg_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ikbkg_ikbkg_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ikbkg_pseudo_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ikbkg_pseudo_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ikbkg_variants.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ncf1_ncf1_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ncf1_ncf1_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ncf1_pseudo_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ncf1_pseudo_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ncf1_pseudo_hap3.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ncf1_pseudo_hap4.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_ncf1_variants.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_neb_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_neb_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_neb_hap3.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_neb_hap4.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_neb_hap5.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_neb_variants.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_pms2_pms2clhap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_pms2_pms2clhap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_pms2_pms2hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_pms2_variants.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_rccx_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_rccx_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_rccx_hap3.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_rccx_hap4.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_rccx_variants.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_smn1_smn1hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_smn1_smn1hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_smn1_smn2hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_smn1_smn2hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_smn1_variants.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_strc_strc_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_strc_strc_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_strc_strcp1_hap1.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_strc_strcp1_hap2.vcf", "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-paraphase/execution/glob-7de36594e58719f1fda80ab822bbd505/HG004_1_strc_variants.vcf"],
    "sample_analysis.hiphase_haplotags": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.hiphase.haplotags.tsv",
    "sample_analysis.small_variant_gvcf": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.g.vcf.gz.tbi",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-deepvariant/deepvariant/3318cd98-2697-4822-b22f-314512ea4878/call-deepvariant_postprocess_variants/execution/HG004_1.GRCh38.deepvariant.g.vcf.gz"
    },
    "sample_analysis.phased_small_variant_vcf": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.deepvariant.phased.vcf.gz.tbi",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.deepvariant.phased.vcf.gz"
    },
    "sample_analysis.hificnv_maf_bw": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.maf.bw",
    "sample_analysis.hificnv_vcf": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.vcf.gz.tbi",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.vcf.gz"
    },
    "sample_analysis.hificnv_copynum_bedgraph": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hificnv/execution/hificnv.HG004_1.copynum.bedgraph",
    "sample_analysis.svsigs": ["/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-pbsv_discover/shard-0/execution/HG004_1.HG004_1.GRCh38.aligned.svsig.gz"],
    "sample_analysis.phased_sv_vcf": {
      "data_index": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.pbsv.phased.vcf.gz.tbi",
      "data": "/scratch3/projects/003/gideonaw/PacBio_Workflow/HiFi-human-WGS-WDL/workflows/sample_analysis/cromwell-executions/sample_analysis/a38bccc8-d525-45ec-bfa6-ffabf1f5962e/call-hiphase/hiphase/be49d0bc-d331-4242-8ebb-6d6438269620/call-run_hiphase/execution/HG004_1.GRCh38.pbsv.phased.vcf.gz"
    }
  },
  "id": "a38bccc8-d525-45ec-bfa6-ffabf1f5962e"
}
[2024-08-02 15:04:13,20] [info] Workflow polling stopped
[2024-08-02 15:04:13,22] [info] 0 workflows released by cromid-a49ef17
[2024-08-02 15:04:13,22] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds
[2024-08-02 15:04:13,23] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds
[2024-08-02 15:04:13,23] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds
[2024-08-02 15:04:13,23] [info] JobExecutionTokenDispenser stopped
[2024-08-02 15:04:13,23] [info] Aborting all running workflows.
[2024-08-02 15:04:13,23] [info] WorkflowStoreActor stopped
[2024-08-02 15:04:13,24] [info] WorkflowLogCopyRouter stopped
[2024-08-02 15:04:13,24] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds
[2024-08-02 15:04:13,24] [info] WorkflowManagerActor: All workflows finished
[2024-08-02 15:04:13,24] [info] WorkflowManagerActor stopped
[2024-08-02 15:04:13,42] [info] Connection pools shut down
[2024-08-02 15:04:13,42] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds
[2024-08-02 15:04:13,42] [info] Shutting down JobStoreActor - Timeout = 1800 seconds
[2024-08-02 15:04:13,42] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds
[2024-08-02 15:04:13,42] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds
[2024-08-02 15:04:13,42] [info] SubWorkflowStoreActor stopped
[2024-08-02 15:04:13,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds
[2024-08-02 15:04:13,42] [info] Shutting down IoProxy - Timeout = 1800 seconds
[2024-08-02 15:04:13,42] [info] JobStoreActor stopped
[2024-08-02 15:04:13,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process
[2024-08-02 15:04:13,42] [info] KvWriteActor Shutting down: 0 queued messages to process
[2024-08-02 15:04:13,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process
[2024-08-02 15:04:13,43] [info] IoProxy stopped
[2024-08-02 15:04:13,43] [info] CallCacheWriteActor stopped
[2024-08-02 15:04:13,43] [info] ServiceRegistryActor stopped
[2024-08-02 15:04:13,43] [info] DockerHashActor stopped
[2024-08-02 15:04:13,45] [info] Database closed
[2024-08-02 15:04:13,45] [info] Stream materializer shut down
[2024-08-02 15:04:13,45] [info] WDL HTTP import resolver closed
[2024-08-02 15:04:13,45] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false
[2024-08-02 15:04:13,45] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false
[2024-08-02 15:04:13,45] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false
[2024-08-02 15:04:13,45] [info] Shutting down connection pool: curAllocated=5 idleQueues.size=1 waitQueue.size=0 maxWaitQueueLimit=256 closed=false
